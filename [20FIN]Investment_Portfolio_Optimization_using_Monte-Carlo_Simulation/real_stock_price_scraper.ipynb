{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Environment Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests, traceback\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "import pymysql\n",
    "from sqlalchemy import create_engine\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Daily Stock Price from Naver Finance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Stock Price Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class get_stock_price:\n",
    "    def __init__(self, item_name):\n",
    "        self.item_name = item_name\n",
    "        \n",
    "    def get_date(self):\n",
    "        self.start = datetime.strftime(datetime.today() - timedelta(days=365),'%Y-%m-%d')\n",
    "        self.end = datetime.strftime(datetime.today() ,'%Y-%m-%d')\n",
    "                \n",
    "    def get_company_code(self):\n",
    "        \"\"\"\n",
    "        Get company codes from KRX(Korea Exchange) \n",
    "        \"\"\"\n",
    "        code_df = pd.read_html('http://kind.krx.co.kr/corpgeneral/corpList.do?method=download&searchType=13', header=0)[0]  \n",
    "        # convert into 6 digits code\n",
    "        code_df.종목코드 = code_df.종목코드.map('{:06d}'.format) \n",
    "        # remove unnecessary columns \n",
    "        code_df = code_df[['회사명', '종목코드']] \n",
    "        # change Korean to English\n",
    "        code_df = code_df.rename(columns={'회사명': 'name', '종목코드': 'code'})\n",
    "        self.code = code_df.query(\"name=='{}'\".format(self.item_name))['code'].to_string(index=False).lstrip()\n",
    "        print('Item Code:',self.code)\n",
    "        return self.code\n",
    "    \n",
    "    def get_last_page(self):\n",
    "        \"\"\"\n",
    "        Get the last page of item \n",
    "        \"\"\"\n",
    "        url = 'http://finance.naver.com/item/sise_day.nhn?code='+ self.code\n",
    "        result = requests.get(url) \n",
    "        soup = BeautifulSoup(result.text, 'html.parser')\n",
    "        maxPage=soup.find_all(\"table\",align=\"center\") \n",
    "        lp = maxPage[0].find_all(\"td\",class_=\"pgRR\") \n",
    "        last_page = lp[0].a.get('href').rsplit('&')[1]\n",
    "        last_page = last_page.split('=')[1]\n",
    "        self.last_page = int(last_page)\n",
    "        return self.last_page\n",
    "\n",
    "    def parse_page(self, page):\n",
    "        try:\n",
    "            url = 'http://finance.naver.com/item/sise_day.nhn?code=' + self.code +'&page='+ str(page)\n",
    "            result = requests.get(url)\n",
    "            soup = BeautifulSoup(result.content, 'html.parser')\n",
    "            df = pd.read_html(str(soup.find(\"table\")), header=0)[0].dropna()   \n",
    "            return df \n",
    "        except Exception as e:\n",
    "            traceback.print_exc()\n",
    "        return None\n",
    "    \n",
    "    def get_final_df(self):\n",
    "        \"\"\"\n",
    "        Create a final dataset\n",
    "        \"\"\"\n",
    "        final_df = None\n",
    "        print('****** Start Crwaling ******')\n",
    "        for page in range(1, self.last_page+1):\n",
    "            _df = self.parse_page(page)\n",
    "            _df_filtered = _df[_df['날짜'] >= self.start]\n",
    "            print('Crawling page #{}'.format(page))\n",
    "            if final_df is None:\n",
    "                final_df = _df_filtered\n",
    "            else:\n",
    "                final_df = pd.concat([final_df, _df_filtered])\n",
    "                \n",
    "            if len(_df) > len(_df_filtered) :\n",
    "                print('****** Crwaling Completed ******')\n",
    "                break\n",
    "        \n",
    "        #change column names\n",
    "        final_df = final_df.rename(columns= {'날짜': 'date', '종가': 'close', '전일비': 'diff',\n",
    "                                             '시가': 'open', '고가': 'high', '저가': 'low', '거래량': 'volume'}) \n",
    "        #change data type into int\n",
    "        final_df[['close', 'diff', 'open', 'high', 'low', 'volume']] = final_df[['close', 'diff', 'open', 'high', 'low', 'volume']].astype(int) \n",
    "        #change data type into date\n",
    "        final_df['date'] = pd.to_datetime(final_df['date']) \n",
    "        #sort\n",
    "        final_df = final_df.sort_values(by=['date'], ascending=True) \n",
    "        #filter dataset by end date\n",
    "        final_df =  final_df[final_df['date'] <= self.end]\n",
    "        final_df.set_index('date')\n",
    "        self.final_df = final_df\n",
    "        return self.final_df\n",
    "         \n",
    "    def save_to_csv(self):\n",
    "        \"\"\"\n",
    "        save dataframes to csv files\n",
    "        \"\"\"\n",
    "        start = self.start.replace('-','')\n",
    "        end = self.end.replace('-','')\n",
    "        \n",
    "        path_dir = 'data/{}'.format(datetime.strftime(datetime.today(), '%Y%m%d'))\n",
    "        if not os.path.exists(path_dir):\n",
    "            os.makedirs(path_dir)\n",
    "        path = os.path.join(path_dir, '{item}_{code}_{start}_{end}.csv'.format(item = self.item_name,code=self.code, \n",
    "                                                                               start=start, end=end))\n",
    "        files_present = glob.glob(path)\n",
    "        if not files_present:\n",
    "            self.final_df.to_csv(path, index=False)\n",
    "            print('Succesfully Saved in {}'.format(path))\n",
    "        else:\n",
    "            print(\"WARNING: This file already exists!\")\n",
    "                    \n",
    "    def save_to_mysql(self):\n",
    "        \"\"\"\n",
    "        save dataframes to mysql server\n",
    "        \"\"\"\n",
    "        engine = create_engine(\"mysql+pymysql://root:\"+\"root\"+\"@localhost:3306/stock_price?charset=utf8\", encoding='utf-8')\n",
    "        conn = engine.connect()\n",
    "        print('Succesfully Saved in MySQL Server')\n",
    "        \n",
    "        self.final_df.to_sql(name=('{item}_{today}'.format(item=self.item_name,\n",
    "                                                           today=datetime.strftime(datetime.today(), '%Y%m%d')).lower()), \n",
    "                             con=engine, if_exists='replace', index=False)\n",
    "        conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Item Name: 삼성전자\n",
      "Item Code: 005930\n",
      "****** Start Crwaling ******\n",
      "Crawling page #1\n",
      "Crawling page #2\n",
      "Crawling page #3\n",
      "Crawling page #4\n",
      "Crawling page #5\n",
      "Crawling page #6\n",
      "Crawling page #7\n",
      "Crawling page #8\n",
      "Crawling page #9\n",
      "Crawling page #10\n",
      "Crawling page #11\n",
      "Crawling page #12\n",
      "Crawling page #13\n",
      "Crawling page #14\n",
      "Crawling page #15\n",
      "Crawling page #16\n",
      "Crawling page #17\n",
      "Crawling page #18\n",
      "Crawling page #19\n",
      "Crawling page #20\n",
      "Crawling page #21\n",
      "Crawling page #22\n",
      "Crawling page #23\n",
      "Crawling page #24\n",
      "Crawling page #25\n",
      "Crawling page #26\n",
      "Crawling page #27\n",
      "Crawling page #28\n",
      "Crawling page #29\n",
      "Crawling page #30\n",
      "Crawling page #31\n",
      "Crawling page #32\n",
      "Crawling page #33\n",
      "Crawling page #34\n",
      "Crawling page #35\n",
      "Crawling page #36\n",
      "Crawling page #37\n",
      "Crawling page #38\n",
      "Crawling page #39\n",
      "Crawling page #40\n",
      "Crawling page #41\n",
      "Crawling page #42\n",
      "Crawling page #43\n",
      "****** Crwaling Completed ******\n",
      "Succesfully Saved in data/20200918\\삼성전자_005930_20190919_20200918.csv\n",
      "Succesfully Saved in MySQL Server\n",
      "\n",
      "Item Name: 신풍제약\n",
      "Item Code: 019170\n",
      "****** Start Crwaling ******\n",
      "Crawling page #1\n",
      "Crawling page #2\n",
      "Crawling page #3\n",
      "Crawling page #4\n",
      "Crawling page #5\n",
      "Crawling page #6\n",
      "Crawling page #7\n",
      "Crawling page #8\n",
      "Crawling page #9\n",
      "Crawling page #10\n",
      "Crawling page #11\n",
      "Crawling page #12\n",
      "Crawling page #13\n",
      "Crawling page #14\n",
      "Crawling page #15\n",
      "Crawling page #16\n",
      "Crawling page #17\n",
      "Crawling page #18\n",
      "Crawling page #19\n",
      "Crawling page #20\n",
      "Crawling page #21\n",
      "Crawling page #22\n",
      "Crawling page #23\n",
      "Crawling page #24\n",
      "Crawling page #25\n",
      "Crawling page #26\n",
      "Crawling page #27\n",
      "Crawling page #28\n",
      "Crawling page #29\n",
      "Crawling page #30\n",
      "Crawling page #31\n",
      "Crawling page #32\n",
      "Crawling page #33\n",
      "Crawling page #34\n",
      "Crawling page #35\n",
      "Crawling page #36\n",
      "Crawling page #37\n",
      "Crawling page #38\n",
      "Crawling page #39\n",
      "Crawling page #40\n",
      "Crawling page #41\n",
      "Crawling page #42\n",
      "Crawling page #43\n",
      "****** Crwaling Completed ******\n",
      "WARNING: This file already exists!\n",
      "Succesfully Saved in MySQL Server\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if __name__== \"__main__\":\n",
    "    item_list = ['삼성전자','신풍제약'] \n",
    "    for item_name in item_list:\n",
    "        print('Item Name:',item_name)\n",
    "        MyStock = get_stock_price(item_name=item_name)\n",
    "        MyStock.get_date()\n",
    "        MyStock.get_company_code()   \n",
    "        MyStock.get_last_page()\n",
    "        MyStock.get_final_df()\n",
    "        MyStock.save_to_csv()\n",
    "        MyStock.save_to_mysql()\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
